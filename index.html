<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="TrackVLA: Embodied Visual Tracking in the Wild">
    <meta name="keywords" content="embodied visual tracking, VLA model">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>TrackVLA: Embodied Visual Tracking in the Wild</title>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZYH3N96LN5"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-ZYH3N96LN5');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/slick.css">
    <link rel="stylesheet" href="./static/css/slick-theme.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/slick.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>

    <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
            <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu">
            <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
                <a class="navbar-item" href="https://cs.berkeley.edu/~shah">
                    <span class="icon">
                        <i class="fas fa-home"></i>
                    </span>
                </a>

                <div class="navbar-item has-dropdown is-hoverable">
                    <a class="navbar-link">
                        Papers
                    </a>
                    <div class="navbar-dropdown">
                        <a class="navbar-item" href="https://sites.google.com/view/drive-any-robot">
                            GNM: A General Navigation Model to Drive Any Robot
                        </a>
                        <a class="navbar-item" href="https://general-navigation-models.github.io/vint">
                            ViNT: A Foundation Model for Visual Navigation
                        </a>
                        <a class="navbar-item" href="https://general-navigation-models.github.io/nomad">
                            NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration
                        </a>
                    </div>
                </div>

                <div class="navbar-item has-dropdown is-hoverable">
                    <a class="navbar-link">
                        More Research
                    </a>
                    <div class="navbar-dropdown">
                        <a class="navbar-item" href="https://sites.google.com/view/viking-release">
                            ViKiNG: Kilometer-Scale Navigatin with Geographic Hints
                        </a>
                        <a class="navbar-item" href="https://sites.google.com/view/lmnav">
                            LM-Nav: Navigation with Pre-Trained Language and Vision Models
                        </a>
                        <a class="navbar-item" href="https://sites.google.com/view/fastrlap">
                            FastRLAP: High-Speed Driving via Real-World Online RL
                        </a>
                        <a class="navbar-item" href="https://sites.google.com/view/sacson-review/home">
                            SACSoN: Scalable Autonomous Control for Social Navigation
                        </a>
                        <a class="navbar-item" href="https://robotics-transformer-x.github.io">
                            Open X-Embodiment: Robotic Learning Datasets and RT-X Models
                        </a>
                    </div>
                </div>
            </div>

        </div>
    </nav> -->


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-2 publication-title">
                            <span class="gradient-blue-text">TrackVLA</span>: Embodied Visual Tracking in the Wild
                        </h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://wsakobe.github.io/">Shaoan Wang</a><sup>1, 2</sup>*,</span>
                            <span class="author-block">
                                <a href="https://jzhzhang.github.io/">Jiazhao Zhang</a><sup>1, 2</sup>*,</span>
                            <span class="author-block">
                                <a href="">Minghan Li</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a href="">Jiahang Liu</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a href="">Anqi Li</a><sup>1, 2</sup>,
                            </span><br>
                            <span class="author-block">
                                <a href="">Kui Wu</a><sup>3</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://fangweizhong.xyz/">Fangwei Zhong</a><sup>4</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.coe.pku.edu.cn/teaching/manufacturing/9993.html">Junzhi Yu</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=zh-CN">Zhizheng Zhang</a><sup>2, 5</sup><sup>†</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://hughw19.github.io/">He Wang</a><sup>1, 2, 5</sup><sup>†</sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <!-- <span class="author-block">Institution Name<br>Conference name and year</span> -->
                            <span class="author-block">
                              <sup>1</sup>Peking University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              <sup>2</sup>GalBot&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              <sup>3</sup>Beihang University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
                              <sup>4</sup>Beijing Normal University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              <sup>5</sup>Beijing Academy of Artificial Intelligence
                            </span>
                          </div>
              
                          <div class="is-size-5 publication-authors">
                            *Equal Contribution&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;†Equal Advising
                          </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <span class="link-block">
                                    <a href=""
                                        class="external-link button is-normal is-rounded is-dark" disabled>
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Arxiv</span>
                                    </a>
                                </span>
                                <!-- <span class="link-block">
                                    <a href="./static/trackvla.pdf"
                                        class="external-link button is-normal is-rounded is-dark" >
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span> -->
                                <span class="link-block">
                                    <a href="https://github.com/wsakobe/TrackVLA"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://github.com/wsakobe/EVT-Bench"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i> </span>
                                        <span>Benchmark</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="./static/trackvla.bib"
                                        class="external-link button is-normal is-rounded is-dark" disabled>
                                        <span class="icon">
                                            <i class="fas fa-quote-left"></i> </span>
                                        <span>BibTex</span>
                                    </a>
                                </span>

                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop is-centered has-text-justified is-size-5">
            <div class="hero-body">
                <video id="teaser" disableRemotePlayback autoplay muted loop playsinline fetchpriority="high"
                    poster="./static/videos/teaser.png">
                    <source src="./static/videos/teaser_new.mp4" type="video/mp4">
                </video>
                <p>
                    <strong><em>TrackVLA</em></strong> is a vision-language-action model capable of simultaneous object recognition and visual tracking, trained on a dataset of 1.7 million samples. It demonstrates robust tracking, long-horizon tracking, and cross-domain generalization across diverse challenging environments.
                </p>
            </div>
        </div>
    </section>

    <!-- Results Carousel--> 
    <section class="hero is-light is-small">
        <div class="hero-body">
            <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                    <div class="item item-jingyuan">
                        <video poster="" id="jingyuan" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/jingyuan.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-long">
                        <video poster="" id="long" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/long_horizon.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-dark">
                        <video poster="" id="dark" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/dark.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-grass">
                        <video poster="" id="grass" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/grass.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-office">
                        <video poster="" id="office" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/office.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-robust">
                        <video poster="" id="robust" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/robust_tracking.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-pursuit">
                        <video poster="" id="pursuit" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/pursuit.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-evt1">
                        <img src="./static/videos/evt_1.gif" alt="EVT1" id="evt1" style="width: 100%; height: 100%; object-fit: cover;">
                    </div>
                    <div class="item item-evt2">
                        <img src="./static/videos/evt_2.gif" alt="EVT2" id="evt2" style="width: 100%; height: 100%; object-fit: cover;">
                    </div>
                </div>
            </div>
        </div>
    </section>

    
    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Embodied visual tracking is a fundamental skill in Embodied AI, enabling an agent to follow a specific target in dynamic environments using only egocentric vision. This task is inherently challenging as it requires both accurate target recognition and effective trajectory planning under conditions of severe occlusion and high scene dynamics. Existing approaches typically address this challenge through a modular separation of recognition and planning. In this work, we propose TrackVLA, a Vision-Language-Action (VLA) model that learns the synergy between object recognition and trajectory planning. Leveraging a shared LLM backbone, we employ a language modeling head for recognition and an anchor-based diffusion model for trajectory planning. To train TrackVLA, we construct an Embodied Visual Tracking Benchmark (EVT-Bench) and collect diverse difficulty levels of recognition samples, resulting in a dataset of 1.7 million samples. Through extensive experiments in both synthetic and real-world environments, TrackVLA demonstrates SOTA performance and strong generalizability. It significantly outperforms existing methods on public benchmarks in a zero-shot manner while remaining robust to high dynamics and occlusion in real-world scenarios at 10 FPS inference speed.
                        </p>
                    </div>
                </div>
            </div>

            <!--/ Abstract. -->

            <!-- Paper video. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Summary Video</h2>
                    <div class="publication-video">
                        <iframe src="https://www.youtube.com/embed/gyYLmwlZvI4?rel=0&amp;showinfo=0" frameborder="0"
                            allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column">
                    <h2 class="title is-3">TrackVLA Pipeline</h2>
                    <div class="content has-text-justified">
                        <p>
                            TrackVLA extends video-based VLM/VLA approaches by introducing a parallel prediction branch for both trajectory planning and target recognition.
                            For trajectory planning, TrackVLA organizes online-captured video data, combining historical and current observations, and concatenates them with tracking instructions and a
                            special tracking token. A diffusion transformer then decodes the output tokens from an LLM into waypoints. For recognition tasks, all video frames are encoded identically and processed in a conventional autoregressive manner.
                        </p>
                        <center>
                            <img src="./static/videos/pipeline.png" alt="Pipeline Image" width="80%">
                        </center>
                    </div>
                </div>
            </div>
            <div class="columns is-centered has-text-centered">
                <div class="column">
                    <h2 class="title is-3">Dataset</h2>
                    <div class="content has-text-justified">
                        <p>
                            To train our parallel-branch TrackVLA, we collect a total of 1.7M newly collected samples, including embodied visual tracking and video-based question-answering data.
                        </p>
                        <center>
                            <img src="./static/videos/dataset.png" alt="Pipeline Image" width="80%">
                        </center>
                    </div>
                </div>
            </div>
            <div class="column is-centered has-text-centered">
                <h2 class="title is-3">Long-horizon Tracking</h2>
                <div class="content has-text-justified">
                    <p>
                        TrackVLA is capable of long-horizon tracking in diverse and dynamic environments. It can effectively track targets over long distances while remaining robust against distractors.
                    </p>
                    <!-- <div class="videos-flex"> -->
                    <center>
                        <video poster="" id="indoors-main" autoplay controls muted loop playsinline width="80%">
                            <source src="./static/videos/long_horizon.mp4" type="video/mp4">
                        </video>
                    </center>
                    <!-- </div> -->
                </div>
    </section>


    <section class="section">
        <div class="container is-max-desktop is-centered">
            <center>
                <h2 class="title is-3">Comparison with Commercial Tracking UAV</h2>
            </center>
            <br>
            <div class="content has-text-justified">
                <p>
                    We conducted a series of experiments to compare the tracking performance of TrackVLA with that of a state-of-the-art commercial tracking UAV based on a modular approach. As shown in the video, TrackVLA performs better in challenging scenarios such as target occlusion and fast motion, thanks to its powerful target reasoning capabilities.
                </p>
                <!-- <div class="videos-flex"> -->
                <center>
                    <video poster="" id="go1-inside-2-main" autoplay controls muted loop playsinline width="80%">
                        <source src="./static/videos/comparison.mp4" type="video/mp4">
                    </video>
                </center>
                <!-- </div> -->
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop is-centered">
            <center>
                <h2 class="title is-3">Environmental Reasoning</h2>
            </center>
            <br>
            <div class="content has-text-justified">
                <p>
                    TrackVLA is capable of reasoning about the environment, enabling it to autonomously recognize traversable areas, avoid obstacles, and generalize to fast-motion and low-illumination scenarios without requiring additional training data.
                </p>
                <div class="videos-flex" style="display: flex; justify-content: space-between; gap: 1rem; flex-wrap: wrap;">
                    <video poster="" autoplay controls muted loop playsinline width="32%">
                        <source src="./static/videos/robust_tracking.mp4" type="video/mp4">
                    </video>
                    <video poster="" autoplay controls muted loop playsinline width="32%">
                        <source src="./static/videos/pursuit.mp4" type="video/mp4">
                    </video>
                    <video poster="" autoplay controls muted loop playsinline width="32%">
                        <source src="./static/videos/dark.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop is-centered">
            <center>
                <h2 class="title is-3">Cross-domain Generalization</h2>
            </center>
            <br>
            <div class="content has-text-justified">
                <p>
                    TrackVLA is capable of cross-domain generalization, enabling robust tracking across diverse scene styles, viewpoints, and camera parameters without additional adaptation.
                </p>
                <!-- <div class="videos-flex"> -->
                <center>
                    <video poster="" id="go1-inside-2-main" autoplay controls muted loop playsinline width="80%">
                        <source src="./static/videos/passive.mp4" type="video/mp4">
                    </video>
                </center>
                <!-- </div> -->
            </div>
        </div>
    </section>

    <!-- <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@article{wang2025trackvla,
                    author  = {Ajay Sridhar and Dhruv Shah and Catherine Glossop and Sergey Levine},
                    title   = {{NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration}},
                    journal = {arXiv pre-print},
                    year    = {2025},
                    url     = {https://arxiv.org/abs/2310.07896}
                  }</code></pre>
        </div>
    </section> -->

    <br>
    <center class="is-size-10">
      The website (<a href="https://github.com/wsakobe/TrackVLA-web">source code</a>) design was adapted from <a href="https://nerfies.github.io" class="external-link"><span
                class="dnerf">Nerfies</span></a>.
    </center>
    <br>
</body>

</html>
