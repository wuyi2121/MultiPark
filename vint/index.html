<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="A foundation model for visual navigation that generalizes across environments and robots, and can be readily adapted to downstream tasks.">
  <meta name="keywords" content="ViNT, robotics, foundation model">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ViNT: A Foundation Model for Visual Navigation</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZYH3N96LN5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-ZYH3N96LN5');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="../static/css/bulma.min.css">
  <link rel="stylesheet" href="../static/css/slick.css">
  <link rel="stylesheet" href="../static/css/slick-theme.css">
  <link rel="stylesheet" href="../static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="../static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="../static/js/fontawesome.all.min.js"></script>
  <script src="../static/js/slick.min.js"></script>
  <script src="../static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://general-navigation-models.github.io/">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://sites.google.com/view/drive-any-robot">
              GNM: A General Navigation Model to Drive Any Robot
            </a>
            <a class="navbar-item" href="https://general-navigation-models.github.io/nomad">
              NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration
            </a>
            <a class="navbar-item" href="https://sites.google.com/view/viking-release">
              ViKiNG: Kilometer-Scale Navigatin with Geographic Hints
            </a>
            <a class="navbar-item" href="https://sites.google.com/view/lmnav">
              LM-Nav: Navigation with Pre-Trained Language and Vision Models
            </a>
            <a class="navbar-item" href="https://sites.google.com/view/fastrlap">
              FastRLAP: High-Speed Driving via Real-World Online RL
            </a>
            <a class="navbar-item" href="https://sites.google.com/view/sacson-review/home">
              SACSoN: Scalable Autonomous Control for Social Navigation
            </a>
            <a class="navbar-item" href="https://robotics-transformer-x.github.io">
              Open X-Embodiment: Robotic Learning Datasets and RT-X Models
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">ViNT: A Foundation Model for Visual Navigation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://cs.berkeley.edu/~shah">Dhruv Shah</a><sup>&#8224;</sup>,</span>
              <span class="author-block">
                <a href="http://ajaysridhar.com/">Ajay Sridhar</a><sup>&#8224;</sup>,</span>
              <span class="author-block">
                <a href="https://dashora7.github.io">Nitish Dashora</a><sup>&#8224;</sup>,
              </span> <br>
              <span class="author-block">
                <a href="https://kylesta.ch">Kyle Stachowicz</a>,
              </span>
              <span class="author-block">
                <a href="https://kevin.black">Kevin Black</a>,
              </span>
              <span class="author-block">
                <a href="https://sites.google.com/view/noriaki-hirose/">Noriaki Hirose</a>,
              </span>
              <span class="author-block">
                <a href="https://cs.berkeley.edu/~svlevine">Sergey Levine</a>
              </span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block">UC Berkeley</span>
            </div>
            <br>

            <div class="is-size-5 publication-venue">
              <span class="venue-block"><span class="publication-awards">Oral Talk</span> at Conference on Robot
                Learning (CoRL)
                2023</span> <br>
              <span class="venue-block">Atlanta, Georgia</span> <br>
            </div>
            <br>

            <div class="is-size-6 publication-authors">
              <span class="venue-block"><span class="publication-awards">Live Demo</span> at Conference on Robot
                Learning (CoRL)
                2023</span> <br>
              <span class="venue-block"><span class="publication-awards">Live Demo</span> at Robot Learning Workshop,
                NeurIPS
                2023</span> <br>
              <span class="venue-block"><span class="publication-awards">Oral Talk</span> at Bay Area Machine Learning
                Symposium
                (BayLearn)
                2023</span> <br>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2306.14846" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="http://arxiv.org/abs/2306.14846" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <span class="link-block">
                  <a href="https://youtu.be/6kNex5dJ5sQ" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/PrieureDeSion/visualnav-transformer"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github-alt"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://github.com/robodhruv/visualnav-transformer#data-wrangling"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i> </span>
                    <span>Data</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop is-centered has-text-justified is-size-5">
      <div class="hero-body">
        <video id="teaser" disableRemotePlayback autoplay muted loop playsinline fetchpriority="high"
          poster="./static/images/teaser.jpg">
          <source src="./static/videos/teaser.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </section>

  <!-- Results Carousel -->
  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-bww1">
            <video poster="" id="bww1" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/small/vint_videos_bww1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-go1-outside">
            <video poster="" id="go1-outside" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/small/vint_videos_go1_outside.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-soda3-left">
            <video poster="" id="soda3-left" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/small/vint_videos_soda3_left.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-rfs">
            <video poster="" id="rfs" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/small/vint_videos_rfs.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-go1-inside-1">
            <video poster="" id="go1-inside-1" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/small/vint_videos_go1_inside_1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-go1-inside-2">
            <video poster="" id="go1-inside-2" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/small/vint_videos_go1_inside_2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-soda3-right">
            <video poster="" id="soda3-right" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/small/vint_videos_soda3_right.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              General-purpose pre-trained models ("foundation models") have enabled practitioners to produce
              generalizable solutions for individual machine learning problems with datasets that are significantly
              smaller than those required for learning from scratch. Such models are typically trained on large and
              diverse datasets with weak supervision, consuming much more training data than is available for any
              individual downstream application.
            </p>
            <p>
              In this paper, we describe the Visual Navigation Transformer (ViNT), a <i>foundation model</i> that aims
              to bring the success of general-purpose pre-trained models to vision-based robotic navigation. ViNT is
              trained with a general goal-reaching objective that can be used with any navigation dataset, and employs a
              flexible Transformer-based architecture to learn navigational affordances and enable efficient adaptation
              to a variety of downstream navigational tasks. ViNT is trained on a number of existing navigation
              datasets, comprising hundreds of hours of robotic navigation from a variety of different robotic
              platforms, and exhibits <i>positive transfer</i>, outperforming specialist models trained on singular
              datasets.
            </p>
            <p>
              ViNT can be augmented with diffusion-based subgoal proposals to explore novel environments, and can solve
              kilometer-scale navigation problems when equipped with long-range heuristics. ViNT can also be adapted to
              novel task specifications with a technique inspired by prompt-tuning, where the goal encoder is replaced
              by an encoding of another task modality (e.g., GPS waypoints or routing commands) embedded into the
              same space of goal tokens. This flexibility and ability to accommodate a variety of downstream problem
              domains establishes ViNT as an effective foundation model for mobile robotics.
            </p>
          </div>
        </div>
      </div>

      <!--/ Abstract. -->

      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Summary Video</h2>
          <div class="publication-video">
            <!---TODO Dhruv: put video link here-->
            <iframe src="https://www.youtube.com/embed/6kNex5dJ5sQ?rel=0&amp;showinfo=0" frameborder="0"
              allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!--/ Paper video. -->

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">

        <div class="column">
          <h2 class="title is-3">ViNT Architecture</h2>
          <div class="content has-text-justified">
            <p>
              ViNT uses a Transformer-based architecture to encode (current and past) visual observations and goals
              using an EfficientNet CNN, and predicts temporal distance and normalized actions in an embodiment-agnostic
              manner.
            </p>
            <figure id="architecture">
              <img src="./static/images/vint_architecture.jpg" alt="ViNT architecture" />
            </figure>
          </div>
        </div>

        <!-- <div class="column">
          <h2 class="title is-4">Adaptation</h2>
          <div class="content has-text-justified">
            <p>
              ViNT can be fine-tuned by a mechanism akin to <i>prompt-tuning</i>, to support alternative goal
              modalities.
            </p>
            <figure id="adaptation">
              <img src="./static/images/vint_adaptation.jpg" alt="ViNT adaptation" />
            </figure>
          </div>
        </div> -->

      </div>
      <div class="column is-centered has-text-centered">
        <h2 class="title is-3">Search Overview</h2>
        <div class="content has-text-justified">
          <p>
            ViNT can explore <i>previously unseen</i> environments by employing a topological graph-based global
            planner. An image-to-image diffusion model proposes diverse exploration targets which are spatially grounded
            using ViNT (yellow), and scored using a goal-directed heuristic <i>h</i>. Subgoals are added to the
            topological graph and executed using the ViNT policy.
          </p>
          <img src="./static/images/vint_search_overview.jpg" alt="Overview of heuristic-guided search with ViNT" />
        </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop is-centered">
      <center>
        <h2 class="title is-3">Long-Range Navigation with Context</h2>
      </center>
      <br>
      <div class="content has-text-justified">
        <p>
          ViNT can solve long-range navigation problems when equipped with a long-range heuristic. Here, we show
          ViNT solving a 1.5km navigation problem in a previously unseen environment, using a heuristic that
          estimates the distance to the goal using a pre-trained depth model.
        </p>
        <div class="videos-flex">
          <video poster="" id="go1-inside-2-main" autoplay controls muted loop playsinline>
            <source src="./static/videos/small/vint_videos_go1_inside_1.mp4" type="video/mp4">
          </video>
          <video poster="" id="bww1-main" autoplay controls muted loop playsinline>
            <source src="./static/videos/small/vint_videos_bww1.mp4" type="video/mp4">
          </video>
          <video poster="" id="rfs-main" autoplay controls muted loop playsinline>
            <source src="./static/videos/small/vint_videos_rfs.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <p> To further show the different exploration behaviors supported by ViNT, here we deploy a locobot to explore an
        office floor from the same starting point but with two different position goals to guide the search.

        Starting from the same position, the different goals lead the robot to two different parts of the building,
        and both trajectories succeed in reaching their goals.
      </p> <br>

      <video poster="" id="both-compare" autoplay controls muted loop playsinline width="100%">
        <source src="./static/videos/vint_videos_soda3_both.mp4" type="video/mp4">
      </video>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop is-centered">
      <center>
        <h2 class="title is-3">Adaptation to Downstream Tasks</h2>
      </center>
      <br>
      <div class="content has-text-justified">
        <p>
          Beyond its core functionality as an image goal-conditioned model, the strong navigational priors
          learned by ViNT can be adapted to a variety of downstream tasks, beyond navigating to image goals, by
          fine-tuning part or all of the model in novel environments or with new modalities of data.
        </p>
        <div class="columns is-centered has-text-centered">
          <div class="column is-two-thirds">
            <img src="./static/images/vint_finetuning.jpg" alt="ViNT fine-tuning" width="80%" />
            <div class="content has-text-justified" width="80%">
              ViNT can transfer navigational affordances to novel tasks (40% success in zero-shot), and
              efficiently masters the task (80% success) with less than 1 hour of fine-tuning data. <span
                class="vint">ViNT
                fine-tuning</span> outperforms <span class="baseline">a <i>specialist</i> model trained with 5&#x2715;
                data</span>.
            </div>
          </div>
          <div class="column is-one-thirds is-centered">
            <img src="./static/images/vint_adaptation.jpg" alt="ViNT adaptation" width="60%" />
            <div class="content has-text-justified" width="80%">
              ViNT can easily be <i>adapted</i> to other common forms of goal-specification by learning a mapping from
              the desired goal modality to the ViNT goal token.
            </div>
          </div>
        </div>
        <video poster="" id="carla-main" autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/vint_videos_carla.mp4" type="video/mp4">
        </video>
        <div class="content has-text-justified" width="80%">
          This allows ViNT to be adapted to a variety of new <span class="newrobot">robots</span>, <span
            class="newenv">environments</span>, <span class="newobjective">objectives</span>, and <span
            class="newgoal">goal modalities</span>. One
          such example is the massively out-of-distribution task of <span class="newrobot">controlling a car</span>
          <span class="newenv"> in a simulated urban
            environment</span> (ViNT was only trained on real data), <span class="newobjective">for the task of
            lane-keeping</span>, <span class="newgoal">and with high-level routing
            commands</span> (ViNT was only trained to reach image goals).
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <center>
        <h2 class="title is-3">Emergent Behaviors</h2>
      </center>
      <br>
      <div class="columns is-centered has-text-centered">

        <div class="column">
          <h2 class="title is-4">Implicit Navigation Preferences</h2>
          <div class="content has-text-justified">
            <p>
              ViNT exhibits implicit <i>preferences</i> for following paved roads and narrow hallways while searching
              previously unseen environments, enabling efficient exploration.
            </p>
            <video poster="" id="implicit-preferences" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/small/vint_videos_bww3.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <h2 class="title is-4">Robustness to Dynamic Pedestrians</h2>
          <div class="content is-centered has-text-justified">
            <p>
              ViNT can successfully navigate around a crowd of dynamic pedestrians and reach the goal behind them,
              despite its simple self-supervised training objective.
            </p>
            <video poster="" id="pedestrians" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/small/vint_videos_pedestrian.mp4" type="video/mp4">
            </video>
          </div>
        </div>

      </div>
    </div>


    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@inproceedings{shah2023vint,
          title     = {Vi{NT}: A Foundation Model for Visual Navigation},
          author    = {Dhruv Shah and Ajay Sridhar and Nitish Dashora and Kyle Stachowicz and Kevin Black and Noriaki Hirose and Sergey Levine},
          booktitle = {7th Annual Conference on Robot Learning},
          year      = {2023},
          url       = {https://arxiv.org/abs/2306.14846}
        }</code></pre>
      </div>
    </section>
    <br>
    <center class="is-size-10">
      The website design was adapted from <a href="https://nerfies.github.io" class="external-link"><span
          class="dnerf">Nerfies</span></a>.
    </center>
    <br>
</body>

</html>
